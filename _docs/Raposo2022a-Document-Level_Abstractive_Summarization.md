---
key: Raposo2022a
year: 2022
date: 2022-06-03
reference: G. Raposo, A. Raposo, and A. S. Carmo, “Document-Level Abstractive Summarization,” unpublished, Deep Structured Learning 2021/2022, Instituto Superior Técnico, 2022
note: "This paper was made for an assignment of the Deep Structured Learning 2021/2022 course at Instituto Superior Técnico."
link:
  name: PDF
  url: "docs/Raposo2022a-Document-Level_Abstractive_Summarization.pdf"
---

@Unpublished{Raposo2022a,
  author   = {Gonçalo Raposo and Afonso Raposo and Ana Sofia Carmo},
  note     = {This paper was made for an assignment of the Deep Structured Learning 2021/2022 course at Instituto Superior Técnico.},
  title    = {Document-Level Abstractive Summarization},
  year     = {2022},
  abstract = {The task of automatic text summarization produces a concise and fluent text summary while preserving key information and overall meaning. Recent approaches to document-level summarization have seen significant improvements in recent years by using models based on the Transformer architecture. However, the quadratic memory and time complexities with respect to the sequence length make them very expensive to use, especially with long sequences, as required by document-level summarization. Our work addresses the problem of document-level summarization by studying how efficient Transformer techniques can be used to improve the automatic summarization of very long texts. In particular, we will use the arXiv dataset, consisting of several scientific papers and the corresponding abstracts, as baselines for this work. Then, we propose a novel retrieval-enhanced approach based on the architecture which reduces the cost of generating a summary of the entire document by processing smaller chunks. The results were below the baselines but suggest a more efficient memory a consumption and truthfulness.},
  eprint   = {X},
  keywords = {Document-level summarization; abstractive summarization; efficient transformers; information retrieval},
  url      = {https://arxiv.org/abs/X},
}
