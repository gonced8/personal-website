---
key: Raposo2023
year: 2023
date: 2023-09-14
reference: "G. Raposo, L. Coheur, and B. Martins, “Prompting, Retrieval, Training: An exploration of different approaches for task-oriented dialogue generation,” in Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, S. Stoyanchev, S. Joty, D. Schlangen, O. Dusek, C. Kennington, and M. Alikhani, Eds., Prague, Czechia: Association for Computational Linguistics, Sep. 2023, pp. 400–412. [Online]. Available: https://aclanthology.org/2023.sigdial-1.37"
link:
  name: PDF
  url: "docs/Raposo2023-Prompting_Retrieval_Training_An_exploration_of_different_approaches_for_task-oriented_dialogue_generation.pdf"
---

@InProceedings{Raposo2023,
  author    = {Raposo, Gon{\c{c}}alo and Coheur, Luisa and Martins, Bruno},
  booktitle = {Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue},
  title     = {Prompting, Retrieval, Training: An exploration of different approaches for task-oriented dialogue generation},
  year      = {2023},
  address   = {Prague, Czechia},
  editor    = {Stoyanchev, Svetlana and Joty, Shafiq and Schlangen, David and Dusek, Ondrej and Kennington, Casey and Alikhani, Malihe},
  month     = sep,
  pages     = {400--412},
  publisher = {Association for Computational Linguistics},
  abstract  = {Task-oriented dialogue systems need to generate appropriate responses to help fulfill users{'} requests. This paper explores different strategies, namely prompting, retrieval, and fine-tuning, for task-oriented dialogue generation. Through a systematic evaluation, we aim to provide valuable insights and guidelines for researchers and practitioners working on developing efficient and effective dialogue systems for real-world applications. Evaluation is performed on the MultiWOZ and Taskmaster-2 datasets, and we test various versions of FLAN-T5, GPT-3.5, and GPT-4 models. Costs associated with running these models are analyzed, and dialogue evaluation is briefly discussed. Our findings suggest that when testing data differs from the training data, fine-tuning may decrease performance, favoring a combination of a more general language model and a prompting mechanism based on retrieved examples.},
  url       = {https://aclanthology.org/2023.sigdial-1.37},
}
